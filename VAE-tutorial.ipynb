{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Variational Autoencoders\n",
    "\n",
    "Carl Doersch, Carnegie Mellon / UC Berkeley\n",
    "August 16, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Generative learning의 목적\n",
    "    - 관찰 가능한 random variable $\\boldsymbol{X} \\in \\mathcal{X}$의 PDF $P(\\boldsymbol{X})$로부터 얻어진 데이터 $\\{\\boldsymbol{x_1}, \\boldsymbol{x_2}, ..., \\boldsymbol{x_M}\\}$를 사용하여 $P(\\boldsymbol{X})$를 추론하는 것\n",
    "    - 이를 통해 '그럴싸한' $\\boldsymbol{x} \\in P(\\boldsymbol{X})$를 sampling할 수 있음\n",
    "- $P(\\boldsymbol{X})$를 추론하기 위한 기존의 방법들은 다음과 같은 문제가 존재함:\n",
    "    - Data structure에 대한 strong assumptions이 수반됨\n",
    "    - Severe approximations로 인해 suboptimal models가 얻어질 수 있음\n",
    "    - Computationally expensive inference procedures에 의존함 (e.g. MCMC)\n",
    "- VAE의 경우 (1) weak assumptions를 포함하고 (2) backpropagation을 사용한 training이 빠르며 (3) approximation으로 인한 error가 상대적으로 작음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: Latent Variable Models\n",
    "\n",
    "- Latent variables $\\boldsymbol{Z} \\in \\mathcal{Z}$가 존재함을 가정하고 그 값에 따라 $\\boldsymbol{X}$의 값이 결정된다고 가정하면, 문제가 좀 더 간단해질 수 있음\n",
    "    - e.g. MNIST digits를 0~9 중 하나로 랜덤하게 정해 놓을 경우, 해당 digit에 부합하는 pixels를 생성할 수 있음\n",
    "    - 모든 datapoint $\\boldsymbol{X}$가 각각 어떤 latent variables $\\boldsymbol{z} \\in \\mathcal{Z}$에 대응된다고 가정하며, 이는 PDF $P(\\boldsymbol{z})$를 따른다고 가정함\n",
    "    - 어떤 parameter vector $\\boldsymbol{\\theta} \\in \\Theta$로 구성된 a family of deterministic functions $f(\\boldsymbol{z};\\boldsymbol{\\theta})$가 존재한다고 가정하면:\n",
    "        - $f: \\mathcal{Z} \\times \\Theta \\to \\mathcal{X}$\n",
    "        - $f$는 deterministic이나 $\\boldsymbol{Z}$가 random variable이므로 $f(\\boldsymbol{Z};\\boldsymbol{\\theta})$ 또한 random variable이 됨\n",
    "- LOTP(the Law of Total Probability)에 의해, $P(\\boldsymbol{X})$는 다음과 같이 factorize할 수 있음:\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{X}) = \\int_{\\mathcal{Z}} P(\\boldsymbol{X}|\\boldsymbol{z};\\boldsymbol{\\theta})P(\\boldsymbol{z})d\\boldsymbol{z}\n",
    "\\end{equation}\n",
    "    - 이 때, $f(\\boldsymbol{z};\\boldsymbol{\\theta})$는 $P(\\boldsymbol{X}|\\boldsymbol{z};\\boldsymbol{\\theta})$를 approximate함\n",
    "- 주어진 데이터의 likelihood $P(\\boldsymbol{x_1}, \\boldsymbol{x_2}, ..., \\boldsymbol{x_n};\\boldsymbol{\\theta})$를 maximize하도록 $\\boldsymbol{\\theta}$를 optimize하게 되면, 곧 모든 $\\boldsymbol{z}$에 대하여 현재 주어진 데이터와 '유사한' examples를 얻게 되는 방향으로 $f(\\boldsymbol{z};\\boldsymbol{\\theta})$의 parameters를 학습하게 됨\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\theta^*} = \n",
    "{argmax}_{\\boldsymbol{\\theta}} P(\\boldsymbol{x_1}, \\boldsymbol{x_2}, ..., \\boldsymbol{x_n};\\boldsymbol{\\theta}) \\\\\n",
    "= {argmax}_{\\boldsymbol{\\theta}} \\frac {1}{L} \\sum_{i=i}^{M} \\log P(\\boldsymbol{x_i};\\boldsymbol{\\theta}) \\\\\n",
    "= {argmax}_{\\boldsymbol{\\theta}} \\frac {1}{L} \\sum_{i=i}^{M} \\log \\int_{\\mathcal{Z}} P(\\boldsymbol{x_i}|\\boldsymbol{z};\\boldsymbol{\\theta})P(\\boldsymbol{z})d\\boldsymbol{z}\n",
    "\\end{equation}\n",
    "\n",
    "- 일반적인 VAE에서는 $P(\\boldsymbol{X}|\\boldsymbol{z};\\boldsymbol{\\theta})$가 Gaussian distribution을 따름을 가정함\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{X}|\\boldsymbol{z};\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{X}|f(\\boldsymbol{z};\\boldsymbol{\\theta}), \\sigma^2*I)\n",
    "\\end{equation}\n",
    "    - Gaussian distrubution의 mean을 estimate하는 데 $f(\\boldsymbol{z};\\boldsymbol{\\theta})$을 사용\n",
    "    - Gaussian distribution의 covariance matrix는 $\\sigma^2*I$로 가정\n",
    "- $\\boldsymbol{X}$가 binary random variables인 경우, $P(\\boldsymbol{X}|\\boldsymbol{z};\\boldsymbol{\\theta})$를 Bernoulli distribution을 따른다고 가정할 수 있음\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{X}|\\boldsymbol{z};\\boldsymbol{\\theta}) = {Bern}(f(\\boldsymbol{z};\\boldsymbol{\\theta}))\n",
    "\\end{equation}\n",
    "    - Bernoulli distribution의 success probability $p = f(\\boldsymbol{z};\\boldsymbol{\\theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
